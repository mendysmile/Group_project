{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nç•¶ç”¨æˆ¶é—œæ³¨Line@å¾Œï¼ŒLineæœƒç™¼ä¸€å€‹FollowEventï¼Œ\\n\\næˆ‘å€‘æ¥å—åˆ°ä¹‹å¾Œï¼Œå–å¾—ç”¨æˆ¶å€‹è³‡ï¼Œå°ç”¨æˆ¶ç¶å®šè‡ªå®šç¾©èœå–®ï¼Œæœƒå›å‚³å››å€‹æ¶ˆæ¯çµ¦ç”¨æˆ¶\\n\\néœ€ä¿®æ”¹å…§å®¹\\n#1. token\\n#2. rich_menu_id\\n#3. server_url\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "ç•¶ç”¨æˆ¶é—œæ³¨Line@å¾Œï¼ŒLineæœƒç™¼ä¸€å€‹FollowEventï¼Œ\n",
    "\n",
    "æˆ‘å€‘æ¥å—åˆ°ä¹‹å¾Œï¼Œå–å¾—ç”¨æˆ¶å€‹è³‡ï¼Œå°ç”¨æˆ¶ç¶å®šè‡ªå®šç¾©èœå–®ï¼Œæœƒå›å‚³å››å€‹æ¶ˆæ¯çµ¦ç”¨æˆ¶\n",
    "\n",
    "éœ€ä¿®æ”¹å…§å®¹\n",
    "#1. token\n",
    "#2. rich_menu_id\n",
    "#3. server_url\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "å•Ÿç”¨ä¼ºæœå™¨åŸºæœ¬æ¨£æ¿\n",
    "\n",
    "\"\"\"\n",
    "import uuid\n",
    "# å¼•ç”¨Web Serverå¥—ä»¶\n",
    "from flask import Flask, request, abort\n",
    "\n",
    "# å¾linebot å¥—ä»¶åŒ…è£¡å¼•ç”¨ LineBotApi èˆ‡ WebhookHandler é¡åˆ¥\n",
    "from linebot import (\n",
    "    LineBotApi, WebhookHandler, WebhookParser\n",
    ")\n",
    "\n",
    "# å¼•ç”¨ç„¡æ•ˆç°½ç« éŒ¯èª¤\n",
    "from linebot.exceptions import (\n",
    "    InvalidSignatureError, LineBotApiError\n",
    ")\n",
    "\n",
    "# è¼‰å…¥jsonè™•ç†å¥—ä»¶\n",
    "import json\n",
    "\n",
    "# è¼‰å…¥åŸºç¤è¨­å®šæª”\n",
    "secretFileContentJson=json.load(open(\"./line_secret_key\",'r',encoding=\"utf-8\"))\n",
    "server_url=secretFileContentJson.get(\"server_url\")\n",
    "\n",
    "# è¨­å®šServerå•Ÿç”¨ç´°ç¯€\n",
    "app = Flask(__name__,static_url_path = \"/images\" , static_folder = \"./images/\")\n",
    "\n",
    "# ç”Ÿæˆå¯¦é«”ç‰©ä»¶\n",
    "line_bot_api = LineBotApi(secretFileContentJson.get(\"channel_access_token\"))\n",
    "handler = WebhookHandler(secretFileContentJson.get(\"secret_key\"))\n",
    "\n",
    "# å•Ÿå‹•serverå°å¤–æ¥å£ï¼Œä½¿Lineèƒ½ä¸Ÿæ¶ˆæ¯é€²ä¾†\n",
    "@app.route(\"/\", methods=['POST'])\n",
    "def callback():\n",
    "    # get X-Line-Signature header value\n",
    "    signature = request.headers['X-Line-Signature']\n",
    "\n",
    "    # get request body as text\n",
    "    body = request.get_data(as_text=True)\n",
    "    app.logger.info(\"Request body: \" + body)\n",
    "\n",
    "    # handle webhook body\n",
    "    try:\n",
    "        handler.handle(body, signature)\n",
    "    except InvalidSignatureError:\n",
    "        abort(400)\n",
    "    except LineBotApiError:\n",
    "        return HttpResponseBadRequest()\n",
    "    return 'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from h5py) (1.17.3)\n",
      "Requirement already satisfied: six in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: keras==2.3.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (5.1.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (1.17.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras==2.3.0) (1.3.2)\n",
      "Requirement already satisfied: tensorflow==1.13.1 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.13.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (3.11.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.25.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (1.17.3)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (42.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\big data\\pycharmprojects\\py\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "é¡åº¦é æ¸¬æ¨¡å‹\n",
    "\n",
    "\"\"\"\n",
    "!pip install h5py\n",
    "!pip install keras==2.3.0\n",
    "!pip install tensorflow==1.13.1\n",
    "from keras.datasets import boston_housing\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "def inputs(list_1):\n",
    "    for n,i in enumerate(list_1):\n",
    "        try:\n",
    "            list_1[n]=int(i)\n",
    "        except:\n",
    "            list_1[n]=0\n",
    "    mean = [26.9859331, 30.2746547, 0.467701106, 741277.863,0.582340575, 2.10021023]\n",
    "    std = [4.61549746, 47.8120109, 0.807741531, 1397767.31,0.493173428, 0.626818273] \n",
    "    userdf = pd.DataFrame(columns=[\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"])\n",
    "    userdf.loc[0] = list_1\n",
    "    userdf-= mean\n",
    "    userdf/= std\n",
    "    model = load_model('predict.h5')\n",
    "    preds = model.predict(userdf)\n",
    "#     print(preds)\n",
    "    #print(max(preds))\n",
    "    pp=np.argmax(preds)\n",
    "#     print(preds.item(np.argmax(preds)))    \n",
    "    list_2=['2è¬~4.5è¬','4.5è¬~9.5è¬','9.5è¬~19.5è¬','19.5è¬~29.5è¬','29.5è¬ä»¥ä¸Š']\n",
    "    return str(list_2[pp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def managePredict(event, mtext):  #è™•ç†LIFFå‚³å›çš„FORMè³‡æ–™  \n",
    "    flist = mtext[3:].split('/')  #å»é™¤å‰ä¸‰å€‹ã€Œ#ã€å­—å…ƒå†åˆ†è§£å­—ä¸²\n",
    "    flist[1]=str(int(flist[1])*12+int(flist[2]))\n",
    "    flist[4]=str(int(flist[4])*10000)\n",
    "    item1 = int(flist[0])  #å–å¾—è¼¸å…¥è³‡æ–™\n",
    "    item2 = int(flist[1])\n",
    "    item3 = int(flist[3])\n",
    "    item4 = int(flist[4])\n",
    "    item5 = int(flist[5])\n",
    "    item6 = int(flist[6])\n",
    "#     text1 = \"æ‚¨è¼¸å…¥çš„è³‡æ–™å¦‚ä¸‹ï¼š\"\n",
    "#     text1 += \"\\nå¹´é½¡ï¼š\"+ flist[0]\n",
    "#     text1 += \"\\nå¹´è³‡ï¼š\"+ flist[1] +\"å€‹æœˆ\"\n",
    "#     text1 += \"\\nå¹´è–ªï¼š\"+ flist[4] +\"å…ƒ\"\n",
    "#     text1 += \"\\nè·æ¥­ï¼š\"+ flist[6]\n",
    "#     text1 += \"\\næœ‰ç„¡è²¸æ¬¾ï¼š\"+ flist[3]\n",
    "#     text1 += \"\\næœ‰ç„¡æŒå¡ï¼š\"+ flist[5]\n",
    "    list_1=[item1, item2, item3, item4, item5, item6]\n",
    "    quota=inputs(list_1)\n",
    "    text1 = \"æ‚¨çš„é ä¼°é¡åº¦ç‚ºï¼š\"+ quota\n",
    "    try:\n",
    "        message = TextSendMessage(  #é¡¯ç¤ºè³‡æ–™\n",
    "            text = text1\n",
    "        )\n",
    "        line_bot_api.reply_message(event.reply_token, message)\n",
    "    except:\n",
    "        line_bot_api.reply_message(event.reply_token, TextSendMessage(text='ç™¼ç”ŸéŒ¯èª¤ï¼')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "æ–°èæ¨è–¦æ¨¡å‹\n",
    "1.è¼‰å…¥æª”æ¡ˆ,é€™å€‹æª”æ¡ˆæ˜¯æ¯ç¯‡æ–‡ç« çš„å…§æ–‡èˆ‡å¹³å‡å‘é‡,æ‹¿å‡ºå¹³å‡å‘é‡é‚£æ¬„,å­˜åˆ°articles_matrixè£¡é¢\n",
    "2.è¼‰å…¥binæª”,é€™å€‹æª”æ¡ˆæ˜¯è¨“ç·´å¥½çš„binæª”,è£¡é¢æœ‰æ¯å€‹è©çš„å‘é‡\n",
    "æ¸¬è©¦é–‹å§‹\n",
    "3.è¼¸å…¥æ–‡å­—:def è«‹è¼¸å…¥æ–‡å­—(input_words)æŠŠè¼¸å…¥çš„æ–‡ç« æ–·è©æˆå­—\n",
    "4.ç²å–è¼¸å…¥è©çš„å¹³å‡å‘é‡:def get_article_avgvector(wordlist):æŠŠæ–·å®Œè©çš„å–®å­—åˆ©ç”¨å…ˆå‰è¼‰å…¥çš„binæª”ä¾åºè®Šæˆå‘é‡,åŠ ç¸½å¾Œå†å–å¹³å‡\n",
    "æ–¼æ˜¯ç¾åœ¨æœ‰äº†è¼¸å…¥çš„æ–‡ç« å¹³å‡è©å‘é‡,æ¥è‘—,åªè¦èˆ‡åŸæœ¬çš„æ–‡ç« å‘é‡åšæ¯”å°,å°±å¯ä»¥æ‰¾åˆ°æœ€ç›¸è¿‘çš„æ–‡ç« \n",
    "5.é¤˜å¼¦ç›¸ä¼¼åº¦:def cos_sim(vector_a, vector_b):é€™å€‹å‡½æ•¸æŠŠè¼¸å…¥çš„å‘é‡è·Ÿ5000ç¯‡æ–°èé€ä¸€æ¯”å°é¤˜å¼¦ç›¸ä¼¼åº¦\n",
    "6.æ¯”å°:def é¤˜é–’ç›¸ä¼¼åº¦æ‰¾æ–‡ç« (input_words):é€™å€‹å‡½æ•¸æŠŠ5.æ¯”å°çš„çµæœå­˜èµ·ä¾†,å†æ‰¾å‡ºæœ€ç›¸è¿‘çš„ä¸€ç¯‡åˆ—å‡ºä¾†\n",
    "\n",
    "\"\"\"\n",
    "!pip install numba\n",
    "from gensim.models import word2vec\n",
    "import gensim.models\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit# åŠ é€Ÿå¥—ä»¶\n",
    "import glob\n",
    "import os\n",
    "import statistics as stat\n",
    "from scipy import stats\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import numpy as ndarray\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import jieba\n",
    "from gensim.test.utils import datapath\n",
    "#0.æ’ˆå‡ºå­˜åœ¨excelçš„å‘é‡è½‰ç‚ºmatrix\n",
    "def get_article_matrix(article, i):\n",
    "    aa = article.loc[:,[\"article_vector_matrix\"]][i:i+1]\n",
    "    #è½‰ARRAYå†è½‰list\n",
    "    b = np.array(aa)\n",
    "    b = b[0].tolist()#list\n",
    "    # åˆ‡\n",
    "    c = str(b[0]).split(',')\n",
    "    article_matrix = np.mat(c).astype(float)\n",
    "    return(article_matrix)\n",
    "\n",
    "#1.è¼‰å…¥æª”æ¡ˆ\n",
    "article = pd.read_excel(r\"./article_news_vector _final.xlsx\")\n",
    "articles_matrix = [get_article_matrix(article,i) for i in range(5000)]\n",
    "\n",
    "#2.è¼‰å…¥binæª”\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(r'C:\\\\Users\\\\Big data\\\\PycharmProjects\\\\Py\\\\linebot_groupproject\\\\100win20min_count3cbow1.bin'), binary=True)\n",
    "\n",
    "\n",
    "#3.è¼¸å…¥æ–‡å­—\n",
    "def please_input_words(rlist):\n",
    "    #æ–·è©\n",
    "    wordlist = jieba.lcut(rlist, cut_all = False)\n",
    "    print(wordlist)\n",
    "    input_vector_matrix = get_article_avgvector(wordlist)\n",
    "    print()\n",
    "    print(\"é€™å¹¾å€‹å­—çš„å¹³å‡å‘é‡æ˜¯:\")\n",
    "    print(input_vector_matrix)\n",
    "    return(input_vector_matrix)\n",
    "\n",
    "\n",
    "#4.ç²å–è¼¸å…¥è©çš„å¹³å‡å‘é‡\n",
    "def get_article_avgvector(wordlist):\n",
    "    #å–æ¯ç¯‡æ–‡ç« å¹³å‡å‘é‡\n",
    "    # x=np.matrix(wv_from_bin[word])å®‰å®‰?\n",
    "    len_wordlist =0\n",
    "    input_avgvector_matrix=0\n",
    "\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            x=np.matrix(wv_from_bin[word])\n",
    "            input_avgvector_matrix+=x\n",
    "            len_wordlist+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        \n",
    "    if type(input_avgvector_matrix) == int:\n",
    "        input_avgvector_matrix = np.matrix(wv_from_bin['è³¼ç‰©'])\n",
    "    else:\n",
    "        input_avgvector_matrix=input_avgvector_matrix/len_wordlist\n",
    "\n",
    "    return(input_avgvector_matrix)\n",
    "\n",
    "\n",
    "#5.é¤˜å¼¦ç›¸ä¼¼åº¦\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param vector_a: å‘é‡ a \n",
    "    :param vector_b: å‘é‡ b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim\n",
    "#æ¸¬è©¦é–‹å§‹_é¤˜å¼¦ç›¸ä¼¼åº¦\n",
    "def manageRecommend(event, mtext):\n",
    "    rlist = mtext[1:]\n",
    "    input_vector_matrix = please_input_words(rlist)\n",
    "    most_similar_article = é¤˜å¼¦ç›¸ä¼¼åº¦æ‰¾æ–‡ç« (rlist, input_vector_matrix)\n",
    "    text_1=str(most_similar_article)\n",
    "    try:\n",
    "        message = TextSendMessage(  #é¡¯ç¤ºè³‡æ–™\n",
    "            text = text_1\n",
    "        )\n",
    "        line_bot_api.reply_message(event.reply_token, message)\n",
    "    except:\n",
    "        line_bot_api.reply_message(event.reply_token, TextSendMessage(text='ç™¼ç”ŸéŒ¯èª¤ï¼')) \n",
    "#æ¯”å°\n",
    "def é¤˜å¼¦ç›¸ä¼¼åº¦æ‰¾æ–‡ç« (rlist, input_vector_matrix):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(5000):\n",
    "        result=cos_sim(input_vector_matrix, articles_matrix[b])\n",
    "        articles_matrix_list.append(result)\n",
    "    print(\"ç¬¬\",articles_matrix_list.index(max(articles_matrix_list)),\"ç¯‡æ–°èæœ€ç›¸ä¼¼\")\n",
    "    most_similar = articles_matrix_list.index(max(articles_matrix_list))\n",
    "    most_similar_article = np.array(article[most_similar:most_similar+1]['content'])[0]\n",
    "    print(\"æ–‡ç« å…§æ–‡ç‚º:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(article[most_similar:most_similar+1]['content'])[0])\n",
    "    return most_similar_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "è£½ä½œæ–‡å­—èˆ‡åœ–ç‰‡çš„æ•™å­¸è¨Šæ¯\n",
    "\n",
    "'''\n",
    "# å°‡æ¶ˆæ¯æ¨¡å‹ï¼Œæ–‡å­—æ”¶å–æ¶ˆæ¯èˆ‡æ–‡å­—å¯„ç™¼æ¶ˆæ¯ å¼•å…¥\n",
    "from linebot.models import (\n",
    "    MessageEvent, TextMessage, TextSendMessage, ImageSendMessage, StickerSendMessage, TemplateSendMessage\n",
    ")\n",
    "\n",
    "#å¼•å…¥æŒ‰éµæ¨¡æ¿\n",
    "from linebot.models.template import(\n",
    "    ButtonsTemplate\n",
    ")\n",
    "\n",
    "# æ¶ˆæ¯æ¸…å–®\n",
    "reply_message_list = [\n",
    "TextSendMessage(text=\"é—œæ³¨ä¿¡æˆ‘å¡ä¾†ï¼Œæ‰¾åˆ°é©åˆä½ çš„å¡ç‰‡ã€‚\"),\n",
    "    TextSendMessage(text=\"å“ˆå›‰ï¼ğŸ˜Šæ­¡è¿åŠ å…¥ä¿¡æˆ‘å¡ä¾†ï¼Œæˆ‘å€‘æä¾›é—œæ–¼ä¿¡ç”¨å¡ğŸ’³çš„å„ç¨®è³‡è¨Šï¼Œæ­¡è¿é»æ“Šæ‚¨æœ‰èˆˆè¶£çš„åŠŸèƒ½å–”ï¼ğŸ˜„\"),\n",
    "    ImageSendMessage(original_content_url='https://%s/images/card.jpg' %server_url ,\n",
    "    preview_image_url='https://%s/images/card-to-woman.jpg' %server_url),\n",
    "    ImageSendMessage(original_content_url='https://%s/images/card_hand.jpg' %server_url,\n",
    "    preview_image_url='https://%s/images/credit-card.jpg' %server_url)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é æ¸¬é¡åº¦æµç¨‹\n",
    "reply_message_list_predict = [\n",
    "TextSendMessage(text=\"æƒ³çŸ¥é“æ‚¨çš„æ ¸å¡é¡åº¦ï¼ŸğŸ¤”è¼¸å…¥ä¸‹åˆ—è¨Šæ¯ï¼Œæˆ‘å€‘å°±æœƒå¹«æ‚¨é æ¸¬å–”ï¼ğŸ˜‰\"),\n",
    "    TemplateSendMessage(\n",
    "     alt_text='Buttons template',\n",
    "      template=ButtonsTemplate(\n",
    "      title='ä¿¡ç”¨å¡æ ¸å¡é¡åº¦',\n",
    "    text='è¼¸å…¥è¨Šæ¯ï¼Œé–‹å§‹é æ¸¬',\n",
    "    actions=[\n",
    "      {\n",
    "        \"type\": \"uri\",\n",
    "        \"label\": \"é–‹å§‹é æ¸¬\",\n",
    "        \"uri\": \"line://app/1653471513-2vnJK4EJ\"\n",
    "      }\n",
    "    ],\n",
    "  )  \n",
    "  )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°èæ¨è–¦æµç¨‹\n",
    "reply_message_list_news = [\n",
    "TextSendMessage(text=\"æ‚¨æƒ³çŸ¥é“å“ªä¸€é¡çš„ä¿¡ç”¨å¡ç›¸é—œè³‡è¨Šå‘¢ï¼Ÿé»é¸ä¸‹æ–¹æŒ‰éˆ•æˆ–æ˜¯è¼¸å…¥@åŠ ä¸Šæ‚¨æ„Ÿèˆˆè¶£çš„å…§å®¹ï¼Œä¾‹å¦‚:@æˆ‘æƒ³çŸ¥é“2020æœ€å¼·ç¥å¡ï¼Œæˆ‘å€‘å°±æœƒæä¾›ç›¸é—œè¨Šæ¯çµ¦æ‚¨ğŸ™‚\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¡ç‰‡æ¨è–¦æµç¨‹\n",
    "reply_message_list_recommend = [\n",
    "TextSendMessage(text=\"ä¸çŸ¥é“å“ªå¼µä¿¡ç”¨å¡é©åˆè‡ªå·±å—ï¼ŸğŸ˜¥è®“æˆ‘å€‘ä¾†å¹«ä½ æŒ‘é¸é©åˆæ‚¨çš„å¡ç‰‡å§ï¼ğŸ¤—\"),\n",
    "    TemplateSendMessage(\n",
    "     alt_text='Buttons template',\n",
    "      template=ButtonsTemplate(\n",
    "      thumbnailImageUrl='https://%s/images/debit-card.png',\n",
    "        title='æŒå¡ç‹€æ³',\n",
    "        text='æ‚¨æ˜¯åˆæ¬¡è¾¦å¡ï¼Ÿé‚„æ˜¯å·²ç¶“æœ‰ä¿¡ç”¨å¡äº†å‘¢ï¼Ÿ',\n",
    "    actions=[\n",
    "      {\n",
    "        \"type\": \"uri\",\n",
    "        \"label\": \"å·²æŒæœ‰ä¿¡ç”¨å¡\",\n",
    "        \"uri\": \"line://app/102\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"uri\",\n",
    "        \"label\": \"åˆæ¬¡è¾¦å¡å°ç™½\",\n",
    "        \"uri\": \"line://app/101\"\n",
    "      }\n",
    "    ],\n",
    "  )  \n",
    "  )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "å»ºç«‹åœ–ç‰‡æ¶ˆæ¯ç´ æ\n",
    "\n",
    "'''\n",
    "\n",
    "# å°‡æ¶ˆæ¯æ¨¡å‹ï¼Œæ–‡å­—æ”¶å–æ¶ˆæ¯èˆ‡æ–‡å­—å¯„ç™¼æ¶ˆæ¯ å¼•å…¥\n",
    "from linebot.models import (\n",
    "    ImageSendMessage\n",
    ")\n",
    "\n",
    "# åœ–ç‰‡æ¶ˆæ¯çš„åŸºæœ¬å»ºæ§‹ç‰¹å¾µ\n",
    "image_message = ImageSendMessage(\n",
    "    original_content_url='https://%s/images/predict.jpg' % server_url,\n",
    "    preview_image_url='https://%s/images/predict.jpg' % server_url\n",
    ")\n",
    "image_message2 = ImageSendMessage(\n",
    "    original_content_url='https://%s/images/news.jpg' % server_url,\n",
    "    preview_image_url='https://%s/images/news.jpg' % server_url\n",
    ")\n",
    "image_message3 = ImageSendMessage(\n",
    "    original_content_url='https://%s/images/recommend.jpg' % server_url,\n",
    "    preview_image_url='https://%s/images/recommend.jpg' % server_url\n",
    ")\n",
    "image_message4 = ImageSendMessage(\n",
    "    original_content_url='https://%s/images/shop.jpg' % server_url,\n",
    "    preview_image_url='https://%s/images/shop.jpg' % server_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "è¨­è¨ˆä¸€å€‹å­—å…¸\n",
    "    ç•¶ç”¨æˆ¶è¼¸å…¥ç›¸æ‡‰æ–‡å­—æ¶ˆæ¯æ™‚ï¼Œç³»çµ±æœƒå¾æ­¤æŒ‘æ€æ¶ˆæ¯\n",
    "\n",
    "'''\n",
    "\n",
    "# æ ¹æ“šè‡ªå®šç¾©èœå–®å››å¼µæ•…äº‹ç·šçš„åœ–ï¼Œè¨­å®šç›¸å°æ‡‰è¨Šæ¯\n",
    "template_message_dict = {\n",
    "    \"[::text:]è«‹å¹«æˆ‘é æ¸¬æ ¸å¡é¡åº¦\":reply_message_list_predict,\n",
    "    \"[::text:]è«‹çµ¦æˆ‘ä¿¡ç”¨å¡ç›¸é—œæ–°è\":reply_message_list_news,\n",
    "    \"[::text:]è«‹å¹«æˆ‘æ¨è–¦ä¿¡ç”¨å¡\":reply_message_list_recommend,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "ç•¶ç”¨æˆ¶ç™¼å‡ºæ–‡å­—æ¶ˆæ¯æ™‚ï¼Œåˆ¤æ–·æ–‡å­—å…§å®¹æ˜¯å¦åŒ…å«[::text:]ï¼Œ\n",
    "    è‹¥æœ‰ï¼Œå‰‡å¾template_message_dict å…§æ‰¾å‡ºç›¸é—œè¨Šæ¯\n",
    "    è‹¥ç„¡ï¼Œå‰‡å›å‚³é è¨­è¨Šæ¯ã€‚\n",
    "\n",
    "'''\n",
    "\n",
    "# ç”¨æˆ¶ç™¼å‡ºæ–‡å­—æ¶ˆæ¯æ™‚ï¼Œ æŒ‰æ¢ä»¶å…§å®¹, å›å‚³æ–‡å­—æ¶ˆæ¯\n",
    "@handler.add(MessageEvent, message=TextMessage)\n",
    "def handle_message(event):\n",
    "    if(event.message.text.find('::text:')!= -1):\n",
    "#         print(event.message.text)\n",
    "        line_bot_api.reply_message(\n",
    "        event.reply_token,\n",
    "        template_message_dict.get(event.message.text)\n",
    "        )\n",
    "    elif event.message.text.find('###')!= -1 and len(event.message.text) > 3:\n",
    "        managePredict(event, event.message.text)\n",
    "    elif event.message.text.find('@')!= -1 and len(event.message.text) > 2:\n",
    "        manageRecommend(event, event.message.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "æ’°å¯«ç”¨æˆ¶é—œæ³¨æ™‚ï¼Œæˆ‘å€‘è¦è™•ç†çš„å•†æ¥­é‚è¼¯\n",
    "\n",
    "1. å–å¾—ç”¨æˆ¶å€‹è³‡ï¼Œä¸¦å­˜å›ä¼ºæœå™¨\n",
    "2. æŠŠå…ˆå‰è£½ä½œå¥½çš„è‡ªå®šç¾©èœå–®ï¼Œèˆ‡ç”¨æˆ¶åšç¶å®š\n",
    "3. å›æ‡‰ç”¨æˆ¶ï¼Œæ­¡è¿ç”¨çš„æ–‡å­—æ¶ˆæ¯èˆ‡åœ–ç‰‡æ¶ˆæ¯\n",
    "\n",
    "'''\n",
    "\n",
    "# è¼‰å…¥Followäº‹ä»¶\n",
    "from linebot.models.events import (\n",
    "    FollowEvent\n",
    ")\n",
    "\n",
    "# è¼‰å…¥requestså¥—ä»¶\n",
    "import requests\n",
    "\n",
    "# å‘ŠçŸ¥handlerï¼Œå¦‚æœæ”¶åˆ°FollowEventï¼Œå‰‡åšä¸‹é¢çš„æ–¹æ³•è™•ç†\n",
    "@handler.add(FollowEvent)\n",
    "def reply_text_and_get_user_profile(event):\n",
    "    \n",
    "    # å–å‡ºæ¶ˆæ¯å…§Userçš„è³‡æ–™\n",
    "    user_profile = line_bot_api.get_profile(event.source.user_id)\n",
    "        \n",
    "     # å°‡ç”¨æˆ¶è³‡è¨Šå­˜åœ¨æª”æ¡ˆå…§\n",
    "    with open(\"../users.txt\", \"a\") as myfile:\n",
    "        myfile.write(json.dumps(vars(user_profile),sort_keys=True))\n",
    "        myfile.write('\\r\\n')  \n",
    "        \n",
    "    # å°‡èœå–®ç¶å®šåœ¨ç”¨æˆ¶èº«ä¸Š\n",
    "    linkRichMenuId=secretFileContentJson.get(\"rich_menu_id\")\n",
    "    linkResult=line_bot_api.link_rich_menu_to_user(secretFileContentJson[\"self_user_id\"], linkRichMenuId)\n",
    "    \n",
    "    # å›è¦†æ–‡å­—æ¶ˆæ¯èˆ‡åœ–ç‰‡æ¶ˆæ¯\n",
    "    line_bot_api.reply_message(\n",
    "        event.reply_token,\n",
    "        reply_message_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [18/Dec/2019 17:59:37] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [18/Dec/2019 17:59:40] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [18/Dec/2019 17:59:42] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "åŸ·è¡Œæ­¤å¥ï¼Œå•Ÿå‹•Serverï¼Œè§€å¯Ÿå¾Œï¼ŒæŒ‰å·¦ä¸Šæ–¹å¡Šï¼Œåœç”¨Server\n",
    "\n",
    "'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', threaded=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
